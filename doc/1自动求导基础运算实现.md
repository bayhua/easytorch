# 1. 自动求导基础运算实现

这部分是自动求导的基础运算实现，注意下面的推导都是使用**微分**进行推导，直接使用导数推导会出现很多问题，比如矩阵求导时链式法则不成立、求得的导数结果为四维张量等等，使用微分就比较合适。具体实现代码见```tensor.py```。

## 1. 基础运算实现

### Add，Sub，Mul，Divide, Pow

最基础的四则运算，共同特点是都是逐元素运算，所以求导都十分简单，与标量形式基本相同，麻烦的是要注意考虑broadcast的问题。

1. $Z = X + Y$，则$dZ = dX + dY$，数学上梯度为单位矩阵，十分简单。比较复杂的是计算机实现中要考虑到向量的broadcast的问题，broadcast分为两种情况：第一种是其中一个矩阵的维度小于另一个，比如$X=[[1, 2], [3, 4]]$，$Y = [1, 2]$，那么在对$Y$求微分时，需要将多余的维度进行$sum$操作；另一种情况是两个的维度相等，但其中一个的某些维度形状为1，比如$X=[[1, 2], [3, 4]]$，$Y = [[1, 2]]$中，$shape(X) = (2, 2)$，$shape(Y)=(1, 2)$，需要对$shape$为1的维度做$sum$操作。

2. Sub可以直接使用Add实现。

3. 逐元素乘法，$Z = X\odot Y$。

4. 逐元素除法，$Z = X / Y$。

5. Pow操作也是逐元素操作，求导比较简单。

### Sum, Mean

都是将矩阵的某一个维度压缩，所以梯度是将当前的梯度广播到原来的size，二者的差别只在于是否需要乘一个常数项。

### Matmul

矩阵乘法或叫矩阵内积，$Z = XY$，对$X$的微分为$dZ = dXY$，所以梯度为$Y^T$，而且要注意左乘右乘的顺序。

一种特殊情况是两个行向量做内积操作时，会自动将第二个行向量转化为列向量，得到一个标量的结果。比如下面的操作也是合法的，会得到5。但目前我还没有想到怎么处理。

``` python
a = Tensor([1., 2.], requires_grad=True)
b = Tensor([1., 2.], requires_grad=True)
c = a @ b
```

### reshape，__getitem__

这两个操作都是对矩阵的元素的重新排列，在数学上是完全无法求梯度的，反向传播的实现中只需要记录梯度，保持梯度与原数据的位置对应。

另外实现要注意这两个操作生成的新向量的数据与原向量的数据是相同的，是浅拷贝。

### Tanh, Relu

这两个激活函数是逐元素函数$\sigma$，设为$y = \sigma(x)$，$dy = \sigma '(x) \odot dx = diag(\sigma '(x)) dx$，第一个等号是通过逐元素操作计算，实现更加简单，但这种操作数学上好像是不存在的，第二个等号是通过矩阵计算，是数学上的正确的形式，但计算机实现会稍微麻烦一点。

Tanh求导：$tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$，然后进行求导。

Relu求导：relu的问题是在$x=0$处不可导，此时需要使用次梯度$c \leq \frac{f(y) - f(x)}{y - x}$，通常取$c=0$。

### Softmax

Softmax的公式为$\frac{e^x}{\sum e^x}$，但这个公式在$x$非常小时，会出现下溢的情况，导致分母为0，所以在实现时，通常会减去最大值$m$，即$\frac{e^{x - m}}{\sum e^{x - m}}$，这样在$x_i=m$时，$e^{x_i-m}$为1，分母一定大于等于1，从而避免了下溢出的问题。

求导时，

## 2. 实现总结

这部分主要是总结理论上不需要考虑，但实际实现时需要考虑的问题。

1. Broadcast问题(详见Add、Sub、Mul、Divide的实现)
2. 两个操作数为同一个对象时，该怎么处理(见```tensor.py:Tensor/backward```的实现)

## 3. 测试

### 代码中加入assert

```backward```操作后，得到的```grad```和```data```的shape是相同的。

### 对拍测试

**未测试的代码永远是错的**。因为API和pytorch完全相同，所以采用和pytorch对拍的方式测试正确性，在backward后比较叶节点的```grad```。
