# 2. 优化器

迭代优化算法的基本框架如下：

1. 计算目标函数对当前参数的梯度$g_t = \nabla f(\omega _t)$
2. 更新历史的一阶动量和二阶动量$m_t$, $V_t$
3. 使用$m_t$控制更新的方向，用$V_t$控制更新的步长，计算当前的下降梯度$\eta_t = \alpha \frac{m_t}{\sqrt{V_t}}$
4. 使用梯度更新$\omega_{t+1} = \omega_t - \eta_t$

不同的优化器就是在第二步中不同。

具体代码见```optim.py```。

## SGD

SGD第二行更新为$m_{t} = g_t$，$V_t = 1$。

优化公式为$\omega_{t+1} = \omega_t - \alpha * g_t$

## Adagrad(Adaptive gradient)

Adagrad第二行更新为$m_{t} = g_{t}$，$V_{t} = V_{t-1} + g_t \odot g_t$加入了自适应的步长，通过累加$V_t$的方式，使得更新梯度$g_t$较大的，更新减慢，而梯度较小的$g_t$，更新加速。

优化公式为$\omega_{t+1} = \omega_t - \frac{\alpha}{\sqrt{V_{t}} + \epsilon} \odot g_t$

Adagrad的问题是在训练后期，由于$V_t$一直在累加，所以分母会过大，导致后期的学习率过小，基本没有变化。

## Moment

Moment引入了动量，第二行的更新为$m_t = \beta m_{t-1} + 
(1 - \beta) g_t$，通过当前梯度和历史梯度的平均，使得在震荡的方向学习减慢，在稳定下降的方向学习加快。

优化公式为$\omega_{t+1} = \omega_t + \alpha * m_t$  

## RMSprop

RMSprop一定程度上解决了Adagrad学习率消失的问题，对二阶动量的更新方式$V_{t} = \beta V_{t-1} + (1 - \beta)g_t \odot g_t$。

优化公式为$\omega_{t+1} = \omega_t - \frac{\alpha}{\sqrt{V_{t}} + \epsilon} \odot g_t$

## Adam

Adam是Adaptive moment，将上面两种的思想结合，第二行的更新公式为$m_t = \beta_0 m_{t-1} + (1 - \beta_0) g_t$，$V_{t} = \beta_1 V_{t-1} + (1-\beta_1)g_t \odot g_t$，然后进行bias correction，$m_t = \frac{m_t}{1 - \beta_0^t}$，$V_t = \frac{V_t}{1 - \beta_1^t}$。

优化公式为$\omega_{t+1} = \omega_t + \alpha * \frac{m_t}{\sqrt{V_t} + \epsilon}$

## 思考

优化器中已经加入了对学习率的衰减，那么再增加学习率衰减还有没有用。
