# 2. 优化器

迭代优化算法的基本框架如下：

1. 计算目标函数对当前参数的梯度$g_t = \nabla f(\omega _t)$
2. 更新历史的一阶动量和二阶动量$m_t$, $V_t$
3. 使用$m_t$控制更新的方向，用$V_t$控制更新的步长，计算当前的下降梯度$\eta_t = \alpha \frac{m_t}{\sqrt{V_t}}$
4. 使用梯度更新$\omega_{t+1} = \omega_t - \eta_t$

不同的优化器就是在第二步中不同。

具体代码见```optim.py```。

## SGD

SGD第二行更新为$m_{t} = g_t$，$V_t = 1$。

优化公式为$\omega_{t+1} = \omega_t - \alpha * g_t$

SGD的优点是实现简单，问题在于

## Adagrad(Adaptive gradient)

Adagrad第二行更新为$m_{t} = g_{t}$，$V_{t} = V_{t-1} + g_t \odot g_t$加入了自适应的步长，通过累加$V_t$的方式，使得更新梯度$g_t$较大的，更新减慢，而梯度较小的$g_t$，更新加速。

优化公式为$\omega_{t+1} = \omega_t - \frac{\alpha}{\sqrt{V_{t} + \epsilon}} \odot g_t$

Adagrad的问题是在训练后期，由于$V_t$一直在累加，所以分母会过大，导致后期的学习率过小，基本没有变化。

## Moment

Moment引入了动量，第二行的更新为$m_t = $

## RMSProp

## Adam
